etcJupyter:
  # Options for Jupyter Notebook itself, set for each user
  jupyter_notebook_config.json: {}

nfsPVC:
  enabled: true
  # If you want a PVC with NFS that can be used to mount home directories
  nfs:
    serverIP: jupyterhub-2i2c-nfs-vm
    shareName: /export/jupyterhub-2i2c-nfs-data-disk-1

jupyterhub:
  cull:
    # Cull only every 30min, not 10
    every: 1800
    # Don't hit the hub API too hard
    concurrency: 1
  prePuller:
    resources:
      requests:
        cpu: 0.0001
        memory: 16Mi
      # Prepuller memoryl imit was too low, so the pods were getting killed constantly
      # https://github.com/jupyterhub/zero-to-jupyterhub-k8s/pull/1764#discussion_r487839915
      limits:
        cpu: 0.0001
        memory: 32Mi
  debug:
    enabled: false
  singleuser:
    imagePullSecret:
      enabled: true
      registry: https://containerregistry2i2cutoronto.azurecr.io
    nodeSelector:
      hub.jupyter.org/pool-name: user-alpha-pool

  auth:
    admin:
      users:
        - yuvipanda@gmail.com
        - choldgraf@gmail.com
    type: azuread
    google:
      loginService: "University of Toronto ID"
  proxy:
    nodeSelector:
      hub.jupyter.org/pool-name: core-pool
    chp:
      resources:
        requests:
          cpu: 0.2
          memory: 128Mi
        limits:
          memory: 512Mi
    traefik:
      resources:
        requests:
          cpu: 0.3
          memory: 256Mi
        limits:
          memory: 512Mi
  hub:
    imagePullSecret:
      enabled: true
      registry: https://containerregistry2i2cutoronto.azurecr.io
    image:
      name: containerregistry2i2cutoronto.azurecr.io/ut-hub
      tag: '0.0.1-n137.h7e1fff3'
    readinessProbe:
      enabled: false
    concurrentSpawnLimit: 100
    consecutiveFailureLimit: 20
    nodeSelector:
      hub.jupyter.org/pool-name: core-pool
    db:
      pvc:
        # Default seems too slow for our database, causes very bad response times
        storageClassName: managed-premium
    initContainers:
      - name: templates-clone
        image: alpine/git
        args:
          - clone
          - --depth=1
          - --single-branch
          - --
          - https://github.com/2i2c-utoronto/homepage
          - /srv/repo
        securityContext:
          runAsUser: 1000
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
        volumeMounts:
          - name: custom-templates
            mountPath: /srv/repo
    extraContainers:
      # Keep templates in sync, so you can autodeploy from master
      - name: templates-sync
        image: alpine/git
        workingDir: /srv/repo
        command:
          - /bin/sh
        args:
          - -c
          # Do git reset --hard origin/master so we aren't confused by force pushes
          - "while true; do git fetch origin; git reset --hard origin/master; sleep 5m; done"
        securityContext:
          runAsUser: 1000
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
        volumeMounts:
          - name: custom-templates
            mountPath: /srv/repo
    extraVolumes:
      - name: custom-templates
        emptyDir: {}
    extraVolumeMounts:
      - mountPath: /usr/local/share/jupyterhub/custom_templates
        name: custom-templates
        subPath: "templates"
      - mountPath: /usr/local/share/jupyterhub/static/extra-assets
        name: custom-templates
        subPath: "extra-assets"
    resources:
      requests:
        cpu: 0.5
        memory: 512Mi
      limits:
        memory: 2Gi
    extraConfig:
      01-working-dir: |
        # Make sure working directory is ${HOME}
        # hubploy has a bug where it unconditionally puts workingdir to be /srv/repo
        c.KubeSpawner.working_dir = '/home/jovyan'
      02-prometheus: |
        # Allow unauthenticated prometheus requests
        # Otherwise our prometheus server can't get to these
        c.JupyterHub.authenticate_prometheus = False
      03-no-setuid: |
        # Disable 'sudo' & similar binaries, regardless of image contents
        c.KubeSpawner.extra_container_config = {
          'securityContext': {
            # Explicitly disallow setuid binaries from working inside the container
            'allowPrivilegeEscalation': False
          }
        }
      04-custom-theme: |
        c.JupyterHub.template_paths = ['/usr/local/share/jupyterhub/custom_templates/']
        # c.JupyterHub.template_vars = {
        #   'hub_title': 'us-central1-b.gcp.pangeo.io',
        #   'hub_': 'a community hub for ocean, atmospheric, and climate research',
        #   'pangeo_welcome': """Welcome to us-central1-b.gcp.pangeo.io. This hub lives in Google Cloud region <code>us-central1-b</code>. It is maintained by the <a href="http://pangeo.io">Pangeo project</a> and supported by a grant from the National Science Foundation (NSF award 1740648), which includes a direct award of cloud credits from Google Cloud. The hub's configuration is stored in the github repository <a href="https://github.com/pangeo-data/pangeo-cloud-federation/">https://github.com/pangeo-data/pangeo-cloud-federation/</a>. To provide feedback and report any technical problems, please use the <a href="https://github.com/pangeo-data/pangeo-cloud-federation//issues">github issue tracker</a>.""",
        # }
      05-azuread: |
        # Email can be easily changed in the utoronto.ca system, so we use the more
        # opaque oid instead.
        # See https://docs.microsoft.com/en-us/azure/active-directory/develop/id-tokens#using-claims-to-reliably-identify-a-user-subject-and-object-id
        c.AzureAdOAuthenticator.username_claim = 'oid'
      06-activity-resolution: |
        # 3 minutes resolution for last_activity tracking
        c.JupyterHub.activity_resolution = 300
        c.JupyterHub.hub_activity_interval = 300
        c.JupyterHub.last_activity_interval = 600

  scheduling:
    userScheduler:
      nodeSelector:
        hub.jupyter.org/pool-name: core-pool
      # FIXME: I *think* this makes user spawns much slower, need to validate
      enabled: false
      resources:
        requests:
          cpu: 0.1
          memory: 512Mi
        limits:
          memory: 512Mi
